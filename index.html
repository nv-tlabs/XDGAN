<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="description" content="XDGAN: Multi-Modal 3D Shape Generation in 2D Space">
  <meta name="keywords"
    content="Generative adversarian network, GAN, 3D, mesh, texture, generative models,  machine learning">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!--link rel="icon" href="assets/nvidia.svg"-->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <title>
    XDGAN: Multi-Modal 3D Shape Generation in 2D Space
  </title>
  <style>
    body {
      font-family: "Titillium Web", "HelveticaNeue-Light",
        "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial,
        "Lucida Grande", sans-serif;
      font-weight: 300;
      font-size: 17px;
      margin-left: auto;
      margin-right: auto;
      max-width: 980px;
      text-align: justify;
    }

    .topnav {
      background-color: #eeeeee;
      overflow: hidden;
    }

    .topnav div {
      max-width: 950px;
      margin: 0 auto;
    }

    .topnav a {
      display: inline-block;
      color: black;
      text-align: center;
      vertical-align: middle;
      padding: 14px 16px;
      text-decoration: none;
      font-size: 16px;
    }

    .topnav img {
      width: 100%;
      margin: 0.5em 0px 0.3em 0px;
    }

    h1 {
      font-weight: 300;
      line-height: 1.15em;
      text-align: center;
    }

    .authors a {
      display: inline-block;
      font-size: 20px;
      padding: 15px;
      text-align: center;
    }

    .authors sup {
      color: #313436;
      font-size: 60%;
    }

    .affil li {
      display: inline-block;
      padding: 10px;
      font-weight: 300;
      text-align: center;
    }

    .venue {
      color: #1367a7;
      text-align: center;
    }

    .venue b {
      font-weight: 300;
    }

    .abstract {
      width: auto;
      font-size: 16px;
      color: #666;
      margin-top: 8px;
      margin-bottom: 8px;
    }

    p.abstract {
      width: 80%;
      line-height: 1.6em;
      margin: auto;
    }

    .caption {
      font-size: 14px;
      color: #666;
      text-align: center;
      margin-top: 8px;
      margin-bottom: 8px;
      line-height: 1.25em;
      width: 80%;
      margin-left: auto;
      margin-right: auto;
    }

    pre {
      background-color: #eee;
      font-size: 14px;
      white-space: pre;
      padding-left: 20px;
      padding-bottom: 10px;
      margin-top: 0px;
    }

    .section .bibtex {
      margin: 32px 0px 32px 0px;
      display: block;
      font-size: 12pt;
      clear: both;
    }

    .screenshot {
      width: 256px;
      border: 1px solid #ddd;
    }

    .flex-row {
      display: flex;
      flex-flow: row wrap;
      justify-content: space-around;
      padding: 0;
      margin: 0;
      list-style: none;
    }
  </style>
</head>

<body style="background-color:#ffffff;">
  <div class="topnav" id="myTopnav">
    <div>
      <a href="https://www.nvidia.com/"><img width="100%" src="imgs/nvidia.svg" /></a>
      <a href="https://nv-tlabs.github.io/"><strong>Toronto AI Lab</strong></a>
    </div>
  </div>
  <div class="title">
    <h1>
      XDGAN: Multi-Modal 3D Shape Generation in 2D Space
    </h1>
  </div>
  <div class="authors" , style="text-align: center;">
    <a href="http://hassanhaija.com" target="_blank">Hassan Abu Alhaija<sup>1</sup></a>
    <a href="https://pch-innovations.com/" target="_blank">Alara Dirik<sup>3</sup></a>
    <a href="https://pch-innovations.com/" target="_blank">André Knörig<sup>3</sup></a>
    <a href="https://www.cs.toronto.edu/~fidler/" target="_blank">Sanja Fidler<sup>1,2</sup></a>
    <a href="https://nv-tlabs.github.io/author/masha-shugrina/" target="_blank">Masha Shugrina<sup>1</sup></a>
  </div>
  <ul class="affil" , style="text-align: center;">
    <li><sup>1</sup>NVIDIA</li>
    <li><sup>2</sup>University of Toronto</li>
    <li><sup>3</sup><a href="https://pch-innovations.com/" target="_blank">PCH Innovations<sup>3</sup></a></li>
  </ul>
  <!-- <p class="venue"><b>NeurIPS 2021</b></p> -->
  <div style="text-align: center; ">
    <!-- <a href="assets/DP_sinkhorn-neurips.pdf " style='padding: 15px;' target="_blank">[Paper PDF]</a> -->
    <a href="https://arxiv.org/abs/2210.03007" style='padding: 15px;'>[Arxiv]</a>
    <!-- <a href="assets/DP_Sinkhorn-supplementary.pdf" style='padding: 15px;' target="_blank">[Supplementary Materials]</a> -->
    <!-- <a href="https://github.com/nv-tlabs/DP-Sinkhorn_code" style='padding: 15px;'>[Code]</a> -->
  </div>
  <div style="text-align: center; ">
    <section id="teaser">
      <!-- <img width="100%" src="imgs/interface_01.gif" style="margin-top: 2em;" /> -->
      <video width="100%" height="auto" controls autoplay muted loop>
        <source src="video/XDGAN_demo_03.mp4" type="video/mp4">
        <!-- <source src="movie.ogg" type="video/ogg"> -->
      Your browser does not support the video tag.
      </video>

      <p class="caption">
        Real-time generation and semantic editing of 3D shapes using XDGAN enabled by our representation of 3D objects
        as geometry images.
      </p>
    </section>
  </div>
  <section id="abstract" style="text-align: center;">
    <h2>Abstract</h2>
    <hr style="width:80%;margin: auto;" />

    <p
      style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666; width: 80%; margin: auto; text-align: justify;">
      Generative models for 2D images has recently seen tremendous progress in quality, resolution and speed as a result
      of the efficiency of 2D convolutional architectures. However it is difficult to extend this progress into the 3D
      domain since most current 3D representations rely on custom network components. This paper addresses a central
      question: Is it possible to directly leverage 2D image generative models to generate 3D shapes instead?
      To answer this, we propose XDGAN, an effective and fast method for applying 2D image GAN architectures to the
      generation of 3D object geometry combined with additional surface attributes, like color textures and normals.
      Specifically, we propose a novel method to convert 3D shapes into compact 1-channel geometry images and leverage
      StyleGAN3 and image-to-image translation networks to generate 3D objects in 2D space.
      The generated geometry images are quick to convert to 3D meshes, enabling real-time 3D object synthesis,
      visualization and interactive editing.
      Moreover, the use of standard 2D architectures can help bring more 2D advances into the 3D realm.
      We show both quantitatively and qualitatively that our method is highly effective at various tasks such as 3D
      shape generation, single view reconstruction and shape manipulation, while being significantly faster and more
      flexible compared to recent 3D generative models.
    </p>
  </section>

  <!-- <section id="paper">
        <h2>Paper and Code</h2>
        <hr />
        <div class="flex-row">
          <div style="box-sizing: border-box; width: 45%; padding: 16px; margin: auto; float: left;">
            <a href="assets/paper_thumbnail.PNG"
              ><img class="screenshot" src="assets/paper_thumbnail.png"
            /></a>
          </div>
          <div style="width: 50%; float: left;">
            <p>
              <b
                >Don't Generate Me:<br />Training Differentially Private
                Generative Models with Sinkhorn Divergence</b
              >
            </p>
            <p>
              Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, Karsten Kreis
            </p>
            <a href="">Paper PDF</a><br />
            <a href="">Code <sup>Coming soon!</sup></a
            ><br />
            <a href="">Video <sup>Coming soon!</sup></a
            ><br />
          </div>
        </div>
        
      </section> -->
  <section id="motivation" style="line-height: 1.75em; text-align: justify; font-size: 12pt; color: #666;">
    <h2>Motivation</h2>
    <hr />
    <div>
      
      <p style="text-indent: 2em">
        Perhaps one of the greatest challenges to the development of generative models for 3D content is converging on
        the right representation. Direct extension of 2D pixel grids to 3D voxel grids suffers
        from high memory demands of 3D convolutions, limiting resolution. Point cloud samples of surface geometry, while
        popular for generative tasks, are limited in their
        ability to model sharp features or high-resolution textures. Recently implicit 3D representations, such as NeRF
        and DeepSDF, as well as hybrids, have shown great promise for
        generative tasks. However, these approaches
        typically are too slow for interactive applications and their output does not interface with existing 3D tools,
        which typically use triangular meshs,
        calling for costly or lossy conversion of models before use. In this work, we
        show that it is possible to directly leverage a 2D GAN architecture designed for images to
        generate high-quality 3D shapes. The key to our approach is parameterization of 3D shapes as 2D planar
        <em>geometry images</em>, which we use as the training dataset for an image-based generator. Our method uses 2D
        generative architectures to produce fixed-topology textured meshes with a single forward pass, and is thus
        immediately practical.
      </p>
    </div>
  </section>
  <section id="geometry images" style="line-height: 1.75em; text-align: justify; font-size: 12pt; color: #666;">
    <h2>Geometry Images</h2>
    <hr />
    <div style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666;">
      <div>
        <img width="100%" src="imgs/Figure_Radials.png" style="margin-top: 2em;" />
        <p class="caption">
          Generation of Rad geometry images from 3D meshes using sphereical projection.
        </p>
      </div>
      <p style="text-indent: 2em"> Geometry images are a representation that captures 3D surface as a 2D array of \((x, y, z)\) values via an implicit surface parameterization, which is a bi-directional mapping from an object's surface to a 2D plane. Such parameterizations are often used in computer graphics to map 2D textures and other attributes onto a 3D surface mesh. Once the parameterization is determined, the plane is sampled to create a 3-channel \(n \times n\) image where each pixel represents an \((x, y, z)\) vertex location of a new geometry image mesh, which is independent from the original object's mesh topology or representation. The order and connectivity of each vertex within this geometry image mesh is encoded by its pixel index, where faces connect neighboring pixel-vertices using a regular structure.<br>
        
      Because projection from the sphere onto a 3D surface has only one degree of freedom (distance along the normal direction of the sphere, see Figure above), we represent the 3D location of a surface point as a single value measuring the distance (which we call “Rad") from the origin along the projection ray, effectively reducing the degrees of freedom of the representation from 3 channels per pixel to only 1. To compute the original \((x, y, z)\) coordinates of all the vertices, one only needs to multiply the Rad image by the image representing the normals of the projection sphere, fixed for the whole dataset. In addition to positions, we produce aligned images of normals and color textures. <br>

      Geometry images can contain additional channels encoding surface properties like color texture, normals, object segmentation and similar. Because the same surface parameterization is used when sampling these additional properties, all channels of the geometry image are aligned, with each pixel representing location and other properties of the same 3D vertex. In this work, we generate normals and texture geometry images and propose applying 2D convolutional generators to these aligned X-channel images. <br>

      </p>
    </div>
  </section>
      
  <section id="method overview" style="line-height: 1.75em; text-align: justify; font-size: 12pt; color: #666;">
    <h2>Method Overview</h2>
    <hr />
    <div style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666;">
      <img width="100%" src="imgs/blank_overview_v2.png" style="margin-top: 2em;" />
      <p class="caption">
        Overview of XDGAN training.
      </p>

      Once our training dataset is converted to Rad geometry images, we train standard 2D GAN architectures to produce "realistic" Rad images. The only modification required is to adapt them to high-precision geometry images instead of the traditional 8-bit RGB images. Specifically, we experiment using StyleGAN3 for geometry generation, but nothing suggests that our approach will not generalize to other GAN architectures. We found training over single-channel Rad images to differ little in its stability from standard image-based GAN training. <br>

      The Normals and Texture images are used  to train an image-to-image translation network to produce X attribute channel images given an input Rad image of the same resolution. Specifically, we experiment with SPADE image-to-image translation network, but nothing in our setup is specific to this particular network choice. We note that SPADE is designed to be able to produce diverse outputs given a single input.

      <img width="100%" src="imgs/inference.png" style="margin-top: 2em;" />
      <p class="caption">
        Overview of XDGAN inference.
      </p>
      During the inference phase, we first generate a Rad geometry image using the StyleGAN3 network. The Rad geometry image is them fed into the image-to-image models which generate the normals and textures. The Rad geometry image is then converted into a mesh using the pixel index as a definition of the vertices connectivity. The normal and texture maps are then applied to the surface to produce a fully texture mesh.  
      
    </div>
  </section>

  <section id="results" style="line-height: 1.75em; text-align: justify; font-size: 12pt; color: #666; align-content: center;">
    <h2>Experimental Results</h2>
    <hr />
    <!-- <div class="flex-row"> -->
      <video width="100%" height="auto" controls autoplay muted loop>
        <source src="video/results_01.m4v" type="video/mp4">
        <!-- <source src="movie.ogg" type="video/ogg"> -->
      Your browser does not support the video tag.
      </video>
      <video width="49%" height="auto" controls autoplay muted loop>
        <source src="video/results_02.m4v" type="video/mp4">
        <!-- <source src="movie.ogg" type="video/ogg"> -->
      Your browser does not support the video tag.
      </video>
      <video width="49%" height="auto" controls autoplay muted loop>
        <source src="video/results_03.m4v" type="video/mp4">
        <!-- <source src="movie.ogg" type="video/ogg"> -->
      Your browser does not support the video tag.
      </video>
    <!-- <img width="100%" src="imgs/results_03.gif" style="margin-top: 2em;" /> -->
    <!-- <img width="48%" src="imgs/results_01.gif" style="margin-top: 2em;" /> -->
    <!-- <img width="48%" src="imgs/results_02.gif" style="margin-top: 2em;" /> -->
    <!-- </div> -->

    <p style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666; text-indent: 2em;">
      A major advantage of our 3D generative model is that it is based on standard GAN architectures, which allows us to directly apply various techniques developed for image-based GANs. To demonstrate this, we integrate the existing technique of InterFaceGAN [Shen et al.(2020b] in order to find editing directions within our trained latent space. To this end, we first project our training shapes into the latent space \(W\) to create a set of of projected latent vectors \(w\) with lables (e.g. sports-car, SUV, etc.). We then train one linear SVMs for each label using these labeled examples, and use the unit normal vector to the separation hyperplane of the SVM  as an editing direction in latent space. The target attribute of a real or generated 3D shape can then be enhanced or negated by simply steering its latent vector \(w\) along this editing direction. 
    </p>
    <video width="100%" height="auto" controls autoplay muted loop>
      <source src="video/editing_01.m4v" type="video/mp4">
      <!-- <source src="movie.ogg" type="video/ogg"> -->
    Your browser does not support the video tag.
    </video>
    <!-- <img width="100%" src="imgs/editing_01.gif" style="margin-top: 2em;" /> -->
    <p class="caption">
      The latent space of XDGAN allows meaningful latent space exploration such as semantic editing.
    </p>
  </section>

  <section id="bibtex" style="line-height: 1.75em; text-align: justify; font-size: 12pt; color: #666;">
    <h2>Citation</h2>
    <hr />
    <p style="line-height: 1.75em; text-align: left; font-size: 12pt; color: #666;">For feedback and questions please
      reach out to <a href="mailto:habualhaija@nvidia.com">Hassan Abu Alhaija</a>. <br>
      If you find this work useful for your research, please consider citing it as:</p>
    <pre><code, style="text-align: left;display: inline-block;">
    @inproceedings{Alhaija2022xdgan,
      title={XDGAN: Multi-Modal 3D Shape Generation in 2D Space},
      author={Alhaija, Hassan Abu and Dirik, Alara and Kn{\"o}rig, Andr{\'e} and Fidler, Sanja and Shugrina, Maria},
    booktitle={British Machine Vision Conference (BMVC 2022)},
    year={2022}
    }
    </code></pre>
  <a name="References">
  </section></a>
  <!-- <section id="Reference" style="text-align: center;">
    <hr>
    <h3>References</h3>
    <p style="line-height: 1.5em; text-align: left; font-size: 12pt; color: #666;">
      [1] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Foundations and Trends in
      Theoretical Computer Science, vol. 9, p. 211–407, Aug. 2014. <br>
      [2] G. Peyr&eacute; and M. Cuturi, “Computational Optimal Transport,” Foundations and Trends in Machine Learning,
      vol. 11, no. 5-6, pp. 355–607, 2019. <br>
      [3] J. Feydy, T. S&eacute;journ&eacute;, F.-X. Vialard, S.-i. Amari, A. Trouv&eacute;, and G. Peyr&eacute;,
      “Interpolating between optimal transport and MMD using sinkhorn divergences,” in The 22nd International Conference
      on Artificial Intelligence and Statistics, pp. 2681–2690, 2019.
    </p>
  </section> -->
</body>

</html>


<!-- <p style="text-align: center;">
        \(W_{c,\lambda}(\mu, \nu)= \min_{\pi \in \Pi} \int c({x},{y})d\pi({x},{y}) + \lambda \int \log\left(
        \frac{d\pi({x},{y})}{d\mu({x})d\nu({y})}\right)d\pi({x},{y}), \)
      </p>
      <p>where \(\pi(x,y)\) is the transport plan defined as \(\Pi = \{ \pi(x,y) \in \mathcal{P}(\mathcal{X} \times
        \mathcal{X})| \int \pi(x, \cdot) d x = \nu, \int \pi(\cdot, y) d y = \mu\}\), with cost function \(c(x,y)\), and
        regularization weight parameter \(\lambda\).</p>
      <p style="text-indent: 2em">
        The Sinkhorn divergence uses auto-correlation terms to reduce the entropic bias introduced by ERWD with respect
        to the exact Wasserstein distance <a href="#References">[3]</a>.
        Empirically computing the Sinkhorn divergence exhibits a bias-variance trade-off. One option is to compute it
        with a single batch of real data \(X\) and a single batch of generated data \(Y\) as:
      <p style="text-align: center;">\(\hat{S}_{c, \lambda}(X, Y) = 2 \hat{W}_\lambda (X, Y) -\hat{W}_\lambda (X, X) -
        \hat{W}_\lambda (Y, Y).\)</p>
      <p>
        This is a biased estimator as it under-estimates the magnitudes of the two
        auto-correlation terms. Alternatively, if we independently draw two batches of real and generated data, we
        obtain an unbiased estimator:
      </p>
      <p style="text-align: center;">
        \(\hat{S}_{c, \lambda}(X, Y, X', Y') = 2\hat{W}_\lambda (X, Y) - \hat{W}_\lambda (X, X') - \hat{W}_\lambda (Y,
        Y').\)
      </p>
      <p style="text-indent: 2em">While unbiased, this formulation adds variance to the mini-batched gradients which can
        be harmful to training. Note that the last term in these two equations affects solely the real data, which is
        not a function of the generator. Hence, this term has no impact on the learning problem.
        We design a semi-debiased formulation that controls the bias-variance tradeoff of Sinkhorn divergence.
        Specifically, the semi-debiased Sinkhorn loss partially re-samples the first batch to create the second batch.
        The amount of variance can be controlled by adjusting the portion of the batch that is re-sampled. The
        semi-debiased Sinkhorn loss is formally defined as below.
      </p>
      <div style="font-style: italic; width: 90%; margin: auto;"> <b>Definition</b>
        (Semi-debiased Sinkhorn Loss) For a mixture fraction \(p \in [0,1]\) and natural number \(n\),
        \(n' = floor(n \times p)\). Given \(n+n'\) generated samples \(X \in \mathcal{X}^{n+n'}\) and \(m\) real samples
        \(Y \in \mathcal{X}^{m}\), the semi-debiased Sinkhorn loss is defined as:
      </div>
      <p style="text-align: center; width: 90%; margin: auto;">
        \(\hat{S}_{c, \lambda, p}(X, Y) = 2 \hat{W}_\lambda (X^{[0:n]}, Y) -\hat{W}_\lambda (X^{[0:n]}, X^{[n':n+n']})\)
      </p>

      <p>
        This semi-debiased loss can be viewed as interpolating between the biased Sinkhorn loss (\(p=0\)) and the fully
        unbiased Sinkhorn loss (\(p=1\)).
      </p>
      <img style="width: 90%;" src="assets/sinkhorn dp diagram 2.png" alt="Diagram of Semi-debiased DP-Sinkhorn" />
      <p class="caption">Flow diagram of DP-Sinkhorn for a single training iteration</p>
      <p style="text-indent: 2em">
        We use the semi-debiased Sinkhorn loss to train generative models. In each iteration, we first generate
        synthetic data and sample real data, with the synthetic data split into two batches to perform debiasing.
        An element-wise cost function that measures the difference between pairs of examples is applied on each element
        of synthetic and real data to form the cost matrix.
        Then, the semi-debiasing Sinkhorn divergence is calculated on these batches.
        Privacy protection is achieved by applying the Gaussian mechanism (a well-known method for adding noise to
        achieve DP) to gradients of the Sinkhorn loss w.r.t generated images.
      </p> -->